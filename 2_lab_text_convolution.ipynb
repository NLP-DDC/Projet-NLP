{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab session is to implement the model proposed by  Yoon Kim, published in 2014. The original paper can be found [here](https://www.aclweb.org/anthology/D14-1181).\n",
    "Of course, there exists pytorch and tensorflow implementations on the web. They are more or less correct and efficient. However, here it is important to do it yourself. The goal is to better understand pytorch and the convolution. \n",
    "\n",
    "The road-map is to: \n",
    "- Implement the convolution and pooling \n",
    "- Add dropout on the last layer\n",
    "\n",
    "To start, it is useful to discover the convolution layers. In this lab, we consider the convolution operation in 1-dimension, followed by the adapted max pooling. \n",
    "\n",
    "\n",
    "We use the same dataset as before: imdb. The first following cells are the same as the previous lab session on this dataset (load the data, build the vocabulary, and prepare data for the model). \n",
    "\n",
    "\n",
    "# Data loading \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch as th\n",
    "import torch.autograd as ag\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "th.manual_seed(1) # set the seed \n",
    "\n",
    "\n",
    "def clean_str(string, tolower=True):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    if tolower:\n",
    "        string = string.lower()\n",
    "    return string.strip()\n",
    "\n",
    "\n",
    "def loadTexts(filename, limit=-1):\n",
    "    \"\"\"\n",
    "    Texts loader for imdb.\n",
    "    If limit is set to -1, the whole dataset is loaded, otherwise limit is the number of lines\n",
    "    \"\"\"\n",
    "    f = open(filename)\n",
    "    dataset=[]\n",
    "    line =  f.readline()\n",
    "    cpt=1\n",
    "    skip=0\n",
    "    while line :\n",
    "        cleanline = clean_str(line).split()\n",
    "        if cleanline: \n",
    "            dataset.append(cleanline)\n",
    "        else: \n",
    "            line = f.readline()\n",
    "            skip+=1\n",
    "            continue\n",
    "        if limit > 0 and cpt >= limit: \n",
    "            break\n",
    "        line = f.readline()\n",
    "        cpt+=1        \n",
    "        \n",
    "    f.close()\n",
    "    print(\"Load \", cpt, \" lines from \", filename , \" / \", skip ,\" lines discarded\")\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load  5000  lines from  /users/Dorian/Documents/M2_MASH/NLP/imdb/imdb.pos  /  0  lines discarded\n",
      "[['excellent'], ['do', \"n't\", 'miss', 'it', 'if', 'you', 'can'], ['a', 'great', 'parody'], ['dreams', 'of', 'a', 'young', 'girl'], ['tromendous', 'piece', 'of', 'art'], ['funny', 'funny', 'movie', '!'], ['need', 'more', 'scifi', 'like', 'this'], ['pride', 'and', 'prejudice', 'is', 'absolutely', 'amazing', '!', '!'], ['scott', 'pilgrim', 'vs', 'the', 'world'], ['quirky', 'and', 'effective']]\n",
      "5000  pos sentences\n",
      "Load  5000  lines from  /users/Dorian/Documents/M2_MASH/NLP/imdb/imdb.neg  /  1  lines discarded\n",
      "[['typical', 'movie', 'where', 'best', 'parts', 'are', 'in', 'the', 'preview'], ['not', 'for', 'the', 'squeamish'], ['cool', 'when', 'i', 'was', 'kid'], ['i', 'appreciate', 'the', 'effort', ',', 'but'], ['pretty', 'bad'], ['much', 'ado', 'about', 'nothing'], ['series', 'of', 'unlikely', 'events'], ['april', 'is', 'the', 'cruelest', 'month'], ['great', 'idea', ',', 'but'], ['and', 'people', 'thought', 'this', 'was', 'good', '\\\\?']]\n",
      "5000  neg sentences\n",
      "7956  words in the vocab\n",
      "10000  sentences\n",
      "50  is maximum sentence length\n",
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "LIM=5000\n",
    "txtfile=\"/users/Dorian/Documents/M2_MASH/NLP/imdb/imdb.pos\"\n",
    "postxt = loadTexts(txtfile,limit=LIM)\n",
    "print(postxt[0:10])\n",
    "print (len(postxt), \" pos sentences\")\n",
    "\n",
    "txtfile=\"/users/Dorian/Documents/M2_MASH/NLP/imdb/imdb.neg\"\n",
    "negtxt = loadTexts(txtfile,limit=LIM)\n",
    "print(negtxt[0:10])\n",
    "\n",
    "print (len(negtxt), \" neg sentences\")\n",
    " \n",
    "\n",
    "## vocabulary selection\n",
    "w2idx = {}\n",
    "\n",
    "\n",
    "maxlength = 0 \n",
    "txtidx = []\n",
    "for sent in postxt+negtxt: \n",
    "    isent = []\n",
    "    maxlength = max(maxlength,len(sent))\n",
    "    for w in sent: \n",
    "        widx=len(w2idx)\n",
    "        if w in w2idx:\n",
    "            widx=w2idx[w]\n",
    "        else :\n",
    "            w2idx[w]=widx\n",
    "        isent.append(widx)\n",
    "    txtidx.append(th.LongTensor(list(set(isent))))\n",
    "    \n",
    "print(len(w2idx), \" words in the vocab\")\n",
    "print(len(txtidx), \" sentences\")\n",
    "print(maxlength, \" is maximum sentence length\")\n",
    "print(txtidx[0])\n",
    "\n",
    "### For the labels\n",
    "labels = th.ones([2*LIM])\n",
    "labels[0:LIM] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings and Convolution layers\n",
    "\n",
    "Unfortunately, an important part of the work is dedicated to playing with dimensions. This is true for pytorch, as well as tensorflow. Here the sequence of operation is \n",
    "- Embedding\n",
    "- Convolution (1D)\n",
    "- Pooling\n",
    "- Linear\n",
    "\n",
    "Moreover, things can be tricky if we want our model to work properly with mini-batch. \n",
    "\n",
    "\n",
    "A quick reminder on Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = 4 # dimension of embeddings, the input size for convolution\n",
    "h2 = 2 # output dimension (filter size) for the convolution\n",
    "embLayer = th.nn.Embedding(num_embeddings=len(w2idx), embedding_dim=h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "torch.Size([7, 4])\n"
     ]
    }
   ],
   "source": [
    "# Don't play with the first sentence, it's only one word ! \n",
    "embs = embLayer(txtidx[1])\n",
    "print(len(txtidx[1]))\n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Look at the documentation of the Conv1d layer. Read it carefully and try to completely understand the following code. \n",
    "For that purpose, we can look at the shapes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1002, -0.6092, -0.9798, -1.6091],\n",
      "        [-0.7121,  0.3037, -0.7773, -0.2515],\n",
      "        [-0.2223,  1.6871,  0.2284,  0.4676],\n",
      "        [-0.6970, -1.1608,  0.6995,  0.1991],\n",
      "        [ 0.8657,  0.2444, -0.6629,  0.8073],\n",
      "        [ 1.1017, -0.1759, -2.2456, -1.4465],\n",
      "        [ 0.0612, -0.6177, -0.7981, -0.1316]], grad_fn=<EmbeddingBackward>)\n",
      "tensor([[[-0.1002, -0.6092, -0.9798, -1.6091, -0.7121,  0.3037, -0.7773],\n",
      "         [-0.2515, -0.2223,  1.6871,  0.2284,  0.4676, -0.6970, -1.1608],\n",
      "         [ 0.6995,  0.1991,  0.8657,  0.2444, -0.6629,  0.8073,  1.1017],\n",
      "         [-0.1759, -2.2456, -1.4465,  0.0612, -0.6177, -0.7981, -0.1316]]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "print(embs)\n",
    "print(embs.view(1,h1,-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embs :  torch.Size([7, 4])\n",
      "tmp  :  torch.Size([1, 4, 7])\n",
      "conv :  torch.Size([1, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "conv1 = th.nn.Conv1d(in_channels=4,out_channels=2,kernel_size=3)\n",
    "tmp=embs.view(1,4,-1)\n",
    "res = conv1(tmp)\n",
    "print(\"embs : \",embs.shape)\n",
    "print(\"tmp  : \",tmp.shape)\n",
    "print(\"conv : \",res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw what happens to better understand the obtained dimensions. \n",
    "\n",
    "Now if we add another parameter for padding (set to 1). What do you observe ? \n",
    "Play a bit with the *kernel_size* along with the *padding* to understand the interaction: \n",
    "- try kernel_size=3,padding=1 and (4,1)\n",
    "- (5,1) and (5,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = th.nn.Conv1d(in_channels=h1,out_channels=h2,kernel_size=3,padding=1)\n",
    "tmp=embs.view(1,4,-1)\n",
    "res = conv1(tmp)\n",
    "print(embs.shape)\n",
    "print(tmp.shape)\n",
    "print(res.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you propose for pooling ? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A first model\n",
    "\n",
    "First we want to create a model with an embedding layer of size 20, followed by a convolution layer: \n",
    "- feature size of 10, \n",
    "- kernel size of 3 (for trigram),\n",
    "- padding set to 1 \n",
    "\n",
    "All these values must be of course parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing this class, allows you to wrap what you have seen so far. Remind the previous lab session, to debug the model, you can first play step-by-step with each layer to ensure you are right with dimensions. Then, write the class and run the training to evaluate the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A lot todo here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Conv1d_classifier(nn.Module):\n",
    "    '''A text classifier:\n",
    "    - input = a list of word indices\n",
    "    - output = probability associated to a binary classification task\n",
    "    - vocab_size: the number of words in the vocabulary we want to embed\n",
    "    - embedding_dim: size of the word vectors\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, feat_size=10, kernel_size=3,lmax=35):\n",
    "        super(Conv1d_classifier, self).__init__()\n",
    "        self.emb = th.nn.Embedding(num_embedding = vocab_size, embedding_dim = embedding_dim)\n",
    "        self.conv = th.nn.Conv1d()\n",
    "            \n",
    "            \n",
    "    def forward(self, input):\n",
    "        # TODO\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State of the art model\n",
    "\n",
    "To have a better model, we should add convolution layers of different kernel size, as in the paper of Yoon Kim 2014. \n",
    "We can use kernels of size 3,5, and 7 for instance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finaly add dropout on the last layer hidden layer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
